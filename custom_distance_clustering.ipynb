{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-12T07:28:51.957166Z",
     "start_time": "2024-07-12T07:28:51.715048Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, make_scorer, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:34.748721Z",
     "start_time": "2024-07-12T09:15:34.743806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "involvement_columns = ['Security', 'Humanities', 'Nat. Sci', 'Health', 'AI Ethics', 'Big Data', \n",
    "                           'Robotics', 'Documents', 'Multimedia', 'NLP', 'KRR', 'Graphs', 'DL/ML', \n",
    "                           'Funding', 'Application-Oriented', 'Number of Members', \n",
    "                           'Academic Collaborations', 'System Maturity', 'Demos', 'Industrial Collaborations']"
   ],
   "id": "42d00e999792d3f7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:36.265127Z",
     "start_time": "2024-07-12T09:15:36.260774Z"
    }
   },
   "cell_type": "code",
   "source": "industry_cols = ['Security', 'Humanities', 'Nat. Sci', 'Health', 'AI Ethics', 'Big Data', 'Robotics', 'Documents', 'Multimedia', 'NLP', 'KRR', 'Graphs', 'DL/ML']",
   "id": "6a6d3c3c15273655",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:37.713406Z",
     "start_time": "2024-07-12T09:15:37.697901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "comp_cols = ['Number of Members', 'Application-Oriented', 'Academic Collaborations', \n",
    "                      'System Maturity', 'Demos', 'Industrial Collaborations']"
   ],
   "id": "a7cee03680929c70",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:38.956796Z",
     "start_time": "2024-07-12T09:15:38.952669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_weights = {\n",
    "    'Security': 1.0, 'Humanities': 1.0, 'Nat. Sci': 1.0, 'Health': 1.0, 'AI Ethics': 1.0, 'Big Data': 1.0, 'Robotics': 1.0, \n",
    "    'Documents': 1.0, 'Multimedia': 1.0, 'NLP': 1.0, 'KRR': 1.0, 'Graphs': 1.0, 'DL/ML': 1.0, \n",
    "    'Number of Members': 0.5, 'Application-Oriented': 0.5, 'Academic Collaborations': 0.5, 'System Maturity': 0.5, \n",
    "    'Demos': 0.5, 'Industrial Collaborations': 0.5\n",
    "}"
   ],
   "id": "33b25d4fefe6655b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:42.590862Z",
     "start_time": "2024-07-12T09:15:42.587118Z"
    }
   },
   "cell_type": "code",
   "source": "weights = {'Strong': 3, 'Good': 2, 'Average': 1, 'None': 0}",
   "id": "c9e832f523762f8",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T07:31:47.975277Z",
     "start_time": "2024-07-12T07:31:47.968737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "expanded_involvement_columns = ['Security', 'Humanities', 'Nat. Sci', 'Health', 'AI Ethics', 'Big Data', \n",
    "                                'Robotics', 'Documents', 'Multimedia', 'NLP', 'KRR', 'Graphs', 'DL/ML', \n",
    "                                'Funding', 'Application-Oriented', 'Cybersecurity', 'Biotech', \n",
    "                                'FinTech', 'Agritech', 'MedTech', 'ClimateTech', 'EdTech', \n",
    "                                'Renewable Energy', 'Telecom', 'E-commerce']\n",
    "granular_strength_map = {f'Level {i}': i for i in range(11)}"
   ],
   "id": "38d0d6d3ead07116",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:44.267959Z",
     "start_time": "2024-07-12T09:15:44.262498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path, index_col=0).transpose()\n",
    "   \n",
    "    for column in involvement_columns:\n",
    "        if column in data.columns:\n",
    "            data[column] = data[column].map(weights).fillna(0)\n",
    "    \n",
    "    return data"
   ],
   "id": "c47d6dfd4815b2e1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:45.764015Z",
     "start_time": "2024-07-12T09:15:45.756384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign_weights(data):\n",
    "    for column, weight in feature_weights.items():\n",
    "        if column in data.columns:\n",
    "            data[column] = data[column]*weight"
   ],
   "id": "aa10aa183dd55e02",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T07:30:34.170662Z",
     "start_time": "2024-07-12T07:30:34.167403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reverse_com_values(data):\n",
    "    max_complementary_value = max(weights.values())\n",
    "    for column in comp_cols:\n",
    "        if column in data.columns:\n",
    "            data[column] = max_complementary_value - data[column]"
   ],
   "id": "f7929ded8ef6501d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:48.886484Z",
     "start_time": "2024-07-12T09:15:48.880104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_elbow_method(X):\n",
    "    inertia = []\n",
    "    K = range(1, 11)\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K, inertia, 'bx-')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "    plt.show()\n",
    "\n",
    "    optimal_clusters = 1\n",
    "    for i in range(1, len(inertia) - 1):\n",
    "        if inertia[i-1] - inertia[i] < inertia[i] - inertia[i+1]:\n",
    "            optimal_clusters = i + 1\n",
    "            break\n",
    "\n",
    "    return optimal_clusters"
   ],
   "id": "890620ef6b012b39",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:51.118500Z",
     "start_time": "2024-07-12T09:15:51.066149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to the CSV file\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "filepath = 'data/synthetic_data.csv'\n",
    "\n",
    "# Load and preprocess the data\n",
    "dfs = load_and_preprocess_data(filepath)\n",
    "\n"
   ],
   "id": "881aa7326a519e46",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:15:53.554532Z",
     "start_time": "2024-07-12T09:15:53.531345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(dfs), columns=dfs.columns, index=dfs.index)"
   ],
   "id": "4ec67ec5e6cce5bd",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:22:23.123981Z",
     "start_time": "2024-07-12T09:21:04.269502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_pairwise_similarity(data):\n",
    " \n",
    "    sim_matrix = np.zeros((len(data), len(data)))\n",
    "    \n",
    "   \n",
    "    scoring_matrix = np.array([\n",
    "        [0, 1, 2, 3],  # None\n",
    "        [1, 2, 3, 4],  # Average\n",
    "        [2, 3, 4, 5],  # Good\n",
    "        [3, 4, 5, 6]   # Strong\n",
    "    ])\n",
    "    \n",
    "    # Map involvement levels to indices\n",
    "    involvement_index = {'None': 0, 'Average': 1, 'Good': 2, 'Strong': 3}\n",
    "    \n",
    "    # Reverse the numerical mapping for correct index lookups\n",
    "    reverse_weights = {0: 'None', 1: 'Average', 2: 'Good', 3: 'Strong'}\n",
    "    \n",
    "    # Calculate pairwise similarity scores\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            if i != j:\n",
    "                similarity_score = 0\n",
    "                for col in industry_cols:\n",
    "                    level_i = data[col].iloc[i]\n",
    "                    level_j = data[col].iloc[j]\n",
    "                    index_i = int(level_i)\n",
    "                    index_j = int(level_j)\n",
    "                    similarity_score += scoring_matrix[index_i][index_j]\n",
    "                sim_matrix[i][j] = similarity_score\n",
    "    \n",
    "    return sim_matrix\n",
    "\n",
    "# Calculate the custom similarity matrix\n",
    "custom_similarity_matrix = calculate_pairwise_similarity(dfs)\n"
   ],
   "id": "652cc1f9729382ee",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:22:46.153151Z",
     "start_time": "2024-07-12T09:22:46.135545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize the custom similarity matrix\n",
    "max_sim = np.max(custom_similarity_matrix)\n",
    "normalized_similarity_matrix = custom_similarity_matrix / max_sim\n",
    "\n",
    "# Convert similarity matrix to distance matrix\n",
    "distance_matrix = 1 - normalized_similarity_matrix\n",
    "\n",
    "# Set the diagonal to zero\n",
    "np.fill_diagonal(distance_matrix, 0)"
   ],
   "id": "1388541d52690e03",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:22:47.666130Z",
     "start_time": "2024-07-12T09:22:47.658826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_agglomerative_clustering(distance_matrix, n_clusters):\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage='complete')\n",
    "    labels = clustering.fit_predict(distance_matrix)\n",
    "    return labels\n",
    "\n",
    "def apply_kmeans_clustering(encoded_data, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "    kmeans.fit(encoded_data)\n",
    "    labels = kmeans.labels_\n",
    "    return labels"
   ],
   "id": "c1b3054d87cff3dd",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:22:50.100133Z",
     "start_time": "2024-07-12T09:22:49.850457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine the optimal number of clusters for Agglomerative Clustering\n",
    "silhouette_avg_scores = [] #FIXME: Use find_optimal_clusters function and evaluate results\n",
    "for n_clusters in range(2, 11):\n",
    "    labels = apply_agglomerative_clustering(distance_matrix, n_clusters)\n",
    "    silhouette_avg = silhouette_score(distance_matrix, labels, metric='precomputed')\n",
    "    silhouette_avg_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_clusters_agg = silhouette_avg_scores.index(max(silhouette_avg_scores)) + 2\n",
    "print(f'Optimal number of clusters for Agglomerative Clustering: {optimal_clusters_agg}')\n",
    "\n",
    "best_labels_agg = apply_agglomerative_clustering(distance_matrix, optimal_clusters_agg)"
   ],
   "id": "b9f775b3916d7458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters for Agglomerative Clustering: 3\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:22:52.749977Z",
     "start_time": "2024-07-12T09:22:52.680885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "silhouette_avg_agg = silhouette_score(distance_matrix, best_labels_agg, metric='precomputed')\n",
    "davies_bouldin_agg = davies_bouldin_score(distance_matrix, best_labels_agg)\n",
    "print(f'Agglomerative - Silhouette Score: {silhouette_avg_agg}, Davies-Bouldin Score: {davies_bouldin_agg}')"
   ],
   "id": "6fc42d07ed0b1ff2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agglomerative - Silhouette Score: 0.28331042364020925, Davies-Bouldin Score: 0.3195661140914868\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:22:55.671257Z",
     "start_time": "2024-07-12T09:22:54.699621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "silhouette_avg_scores = []\n",
    "for n_clusters in range(2, 11):\n",
    "    labels = apply_kmeans_clustering(distance_matrix, n_clusters)\n",
    "    silhouette_avg = silhouette_score(distance_matrix, labels, metric='precomputed')\n",
    "    silhouette_avg_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_clusters_kmeans = silhouette_avg_scores.index(max(silhouette_avg_scores)) + 2\n",
    "print(f'Optimal number of clusters for KMeans: {optimal_clusters_kmeans}')\n",
    "\n",
    "best_labels_kmeans = apply_kmeans_clustering(distance_matrix, optimal_clusters_kmeans)"
   ],
   "id": "c1ef044e35c91cf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters for KMeans: 2\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:22:58.024716Z",
     "start_time": "2024-07-12T09:22:57.986139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "silhouette_avg_kmeans = silhouette_score(distance_matrix, best_labels_kmeans, metric='precomputed')\n",
    "davies_bouldin_kmeans = davies_bouldin_score(distance_matrix, best_labels_kmeans)\n",
    "print(f'KMeans - Silhouette Score: {silhouette_avg_kmeans}, Davies-Bouldin Score: {davies_bouldin_kmeans}')"
   ],
   "id": "64759d3933219e1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans - Silhouette Score: 0.008953567736218861, Davies-Bouldin Score: 0.6392769308855364\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:19:40.446438Z",
     "start_time": "2024-07-12T09:19:40.440036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Choose the best clustering algorithm\n",
    "clustering_scores = {\n",
    "    'KMeans': silhouette_avg_kmeans,\n",
    "    'Agglomerative': silhouette_avg_agg,\n",
    "    #'DBSCAN': best_dbscan_score if len(set(best_labels_dbscan)) > 1 else -1\n",
    "}\n",
    "\n",
    "best_clustering_method = max(clustering_scores, key=clustering_scores.get)\n",
    "print(f'Best Clustering Method: {best_clustering_method}')"
   ],
   "id": "b1a6958d4f1cc719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Clustering Method: KMeans\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:27:10.636255Z",
     "start_time": "2024-07-10T09:27:10.624618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if best_clustering_method == 'KMeans':\n",
    "    best_labels = best_labels_kmeans\n",
    "else:\n",
    "    best_labels = best_labels_agg"
   ],
   "id": "7f491941798dd9cf",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:27:19.167245Z",
     "start_time": "2024-07-10T09:27:19.132810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_scaled['Cluster'] = best_labels\n",
    "cluster_label_series = pd.Series(best_labels)\n",
    "cluster_counts = cluster_label_series.value_counts().sort_index()\n",
    "cluster_counts"
   ],
   "id": "34f68db398f2362a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    538\n",
       "1    462\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:27:24.147462Z",
     "start_time": "2024-07-10T09:27:24.101381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def profile_clusters(df):\n",
    "    \"\"\"\n",
    "    Generates statistical profiles for each cluster.\n",
    "\n",
    "    This function groups the DataFrame by the 'cluster' column and calculates\n",
    "    the mean, standard deviation, minimum, and maximum for each feature within\n",
    "    each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the team data, including a 'cluster' column.\n",
    "\n",
    "    Returns:\n",
    "    - profiles (DataFrame): A DataFrame with the mean, standard deviation, minimum, and maximum\n",
    "                            values for each feature within each cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    profiles = df.groupby('Cluster').agg(['mean', 'std', 'min', 'max'])\n",
    "    return profiles\n",
    "\n",
    "cluster_profiles = profile_clusters(df_scaled)\n",
    "print(\"Cluster Profiles:\")\n",
    "print(cluster_profiles)"
   ],
   "id": "32e46fb46dfef93f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Profiles:\n",
      "Field   Security                               Humanities                      \\\n",
      "            mean       std       min       max       mean       std       min   \n",
      "Cluster                                                                         \n",
      "0        0.00517  0.987479 -0.448676  3.269633   0.100402  1.007252 -0.905336   \n",
      "1       -0.00602  1.016493 -0.448676  3.269633  -0.116919  0.980774 -0.905336   \n",
      "\n",
      "Field              Nat. Sci            ... System Maturity            \\\n",
      "              max      mean       std  ...             min       max   \n",
      "Cluster                                ...                             \n",
      "0        1.977899  0.026982  1.033429  ...       -2.598027  1.042366   \n",
      "1        1.977899 -0.031421  0.960894  ...       -2.598027  1.042366   \n",
      "\n",
      "Field       Demos                               Industrial Collaborations  \\\n",
      "             mean       std       min       max                      mean   \n",
      "Cluster                                                                     \n",
      "0        0.162369  0.803905 -2.317323  1.202662                 -0.069669   \n",
      "1       -0.189079  1.161807 -2.317323  1.202662                  0.081130   \n",
      "\n",
      "Field                                  \n",
      "              std       min       max  \n",
      "Cluster                                \n",
      "0        0.839661 -1.190946  1.918574  \n",
      "1        1.155715 -1.190946  1.918574  \n",
      "\n",
      "[2 rows x 80 columns]\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:01:38.856018Z",
     "start_time": "2024-07-10T12:01:38.665473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df_scaled.drop(columns=['Cluster'])\n",
    "y = df_scaled['Cluster']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "a7347cabd0e8b961",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:01:47.777821Z",
     "start_time": "2024-07-10T12:01:47.755472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QualitativeToQuantitativeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "    A custom transformer to map qualitative descriptions to quantitative values.\n",
    "\n",
    "    Parameters:\n",
    "    - columns (list of str): List of column names to be transformed.\n",
    "    - mapping (dict): A dictionary mapping qualitative descriptions to quantitative values.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y=None): Fits the transformer on the dataset. (No action needed for this transformer)\n",
    "    - transform(X): Transforms the specified columns in the dataset using the provided mapping.\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, columns, mapping):\n",
    "        self.columns = columns\n",
    "        self.mapping = mapping\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer. This transformer doesn't need to learn anything, so fit does nothing.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (DataFrame): Input data.\n",
    "        - y (Series or None): Target data (not used).\n",
    "        \n",
    "        Returns:\n",
    "        - self: Fitted transformer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the specified columns using the provided mapping.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (DataFrame): Input data.\n",
    "        \n",
    "        Returns:\n",
    "        - X_transformed (DataFrame): Transformed data with specified columns mapped to quantitative values.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            if column in X_transformed.columns:\n",
    "                X_transformed[column] = X_transformed[column].map(self.mapping).fillna(0)\n",
    "        return X_transformed\n",
    "\n",
    "weights = {'Strong': 3, 'Good': 2, 'Average': 1, 'None': 0}\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('qualitative', QualitativeToQuantitativeTransformer(involvement_columns, weights), involvement_columns),\n",
    "        ('scaler', StandardScaler(), involvement_columns + comp_cols)\n",
    "    ],\n",
    ")\n",
    "\n"
   ],
   "id": "40eae7f8313fd174",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:01:56.846689Z",
     "start_time": "2024-07-10T12:01:56.826038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n"
   ],
   "id": "c2509014416d18a2",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:02:04.466884Z",
     "start_time": "2024-07-10T12:02:04.459395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': [None, 'sqrt', 'log2']  \n",
    "}\n"
   ],
   "id": "2be1363cfe1ddeb8",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Perform hyperparameter optimization using GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "train_score = best_pipeline.score(X_train, y_train)\n",
    "test_score = best_pipeline.score(X_test, y_test)\n",
    "\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Optimized Train Accuracy: {train_score}\")\n",
    "print(f\"Optimized Test Accuracy: {test_score}\")\n",
    "print(\"Optimized Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Optimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ],
   "id": "2b69a10bd498b0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
